#Â ImportÂ packages
!pipÂ installÂ dmba
importÂ numpyÂ asÂ np
importÂ pandasÂ asÂ pd
importÂ datetime
importÂ matplotlib.pylabÂ asÂ plt
%matplotlibÂ inline

fromÂ sklearn.model_selectionÂ importÂ train_test_split
fromÂ sklearn.metricsÂ importÂ classification_report
fromÂ sklearn.metricsÂ importÂ confusion_matrix,Â ConfusionMatrixDisplay
fromÂ imblearn.over_samplingÂ importÂ SMOTE

#Â XGBoost
fromÂ xgboostÂ importÂ XGBClassifier
fromÂ xgboostÂ importÂ plot_tree

#Â Joblib
importÂ joblib

dataÂ =Â pd.read_excel("DataÂ withÂ applicationÂ questions.xlsx",Â sheet_name='RawÂ Data')

#Â PivotÂ data
initial_HWHÂ =Â data.pivot_table(index='ID',Â columns='Statistic',Â values='Value',aggfunc='first').reset_index()

#Â CreateÂ graduatedÂ andÂ preview
initial_HWH['graduated']Â =Â initial_HWH['graduated'].apply(lambdaÂ x:Â 1Â ifÂ xÂ ==Â 'Yes'Â elseÂ 0)
initial_HWH['Preview']Â =Â initial_HWH['attendance_days'].apply(lambdaÂ x:Â 1Â ifÂ xÂ >Â 12Â elseÂ 0)

#Â ExtractÂ yearÂ andÂ convertÂ dataÂ typesÂ forÂ ageÂ calculation
initial_HWH['attendance_first']Â =Â pd.to_datetime(initial_HWH['attendance_first'],Â format='%Y-%m-%d')
initial_HWH['attendance_first_year']Â =Â initial_HWH['attendance_first'].dt.year

#Â ConvertÂ birth_yearÂ toÂ int
initial_HWH['birth_year']Â =Â pd.to_numeric(initial_HWH['birth_year'],Â errors='coerce').astype('Int64')Â #Â ignoreÂ anyÂ NaNÂ valuesÂ andÂ convertÂ birth_yearÂ toÂ numeric

#Â CalculateÂ age
initial_HWH['Age_Start']Â =Â initial_HWH['attendance_first_year']Â -Â initial_HWH['birth_year']

#Â FilterÂ outÂ anyÂ ageÂ valuesÂ thatÂ areÂ 0Â orÂ 2023.Â (3/1/25Â -Â AddÂ ageÂ rangeÂ forÂ workingÂ toÂ filterÂ outÂ anyÂ incompleteÂ values)
initial_HWHÂ =Â initial_HWH[~initial_HWH.Age_Start.isin([0.0,Â 2023.0])]

application_subsetÂ =Â initial_HWH[["ID","attendance_days","Preview","graduated",'ssf_initial:adult_education',Â 'ssf_initial:child_care',Â 'ssf_initial:community',
Â Â Â Â 'ssf_initial:employment',Â 'ssf_initial:housing',Â 'ssf_initial:income',
Â Â Â Â 'ssf_initial:math_skills',Â 'ssf_initial:mental_health',Â 'ssf_initial:reading_skills',
Â Â Â Â 'ssf_initial:social',Â 'ssf_initial:substance_abuse',Â "Age_Start"]]
application_subsetÂ =Â application_subset.convert_dtypes()

application_subsetÂ =Â application_subset.dropna()

#Â SaveÂ StudentÂ IDÂ columnÂ priorÂ toÂ droppingÂ -Â AllowsÂ usÂ toÂ seeÂ incorrectÂ predictionsÂ byÂ student
student_idsÂ =Â application_subset['ID'].values

#Â DropÂ specificÂ featureÂ variablesÂ includingÂ outcomeÂ variableÂ andÂ setÂ outcomeÂ variable
XÂ =Â application_subset.drop(columns=['ID','attendance_days','graduated','Preview'],Â axis=1).astype(int)
yÂ =Â application_subset['Preview'].astype(int)

#Â Train_test_split
train_X,Â valid_X,Â train_y,Â valid_y,Â id_train,Â id_testÂ =Â train_test_split(
Â Â Â Â X,Â y,Â student_ids,Â test_size=0.3,Â stratify=y,Â random_state=42)Â #Â WeÂ useÂ stratify=yÂ toÂ ensureÂ thatÂ train/testÂ dataÂ representsÂ theÂ trueÂ classÂ distributionÂ (givenÂ weÂ haveÂ moreÂ 0sÂ thanÂ 1s).Â TheÂ ideaÂ isÂ thatÂ stratifyingÂ theÂ sampleÂ combinedÂ withÂ theÂ weightsÂ handlesÂ theÂ classÂ imbalance.

#Â ApplyÂ SMOTEÂ toÂ theÂ trainingÂ data:Â AllowsÂ usÂ toÂ balanceÂ theÂ trainingÂ dataÂ ofÂ thoseÂ whoÂ continueÂ pastÂ 12Â daysÂ (1)Â andÂ doÂ notÂ continueÂ pastÂ 12Â daysÂ (0)
smoteÂ =Â SMOTE(sampling_strategy='minority',Â random_state=1)
train_X_resampled,Â train_y_resampledÂ =Â smote.fit_resample(train_X,Â train_y)


#Â DefineÂ theÂ XGBoostÂ model
xgb_modelÂ =Â XGBClassifier(
Â Â Â Â n_estimators=50,Â #Â TheÂ amountÂ ofÂ treesÂ usedÂ toÂ estimateÂ classÂ 0Â orÂ classÂ 1Â inÂ model
Â Â Â Â max_depth=4,Â Â Â Â Â #Â SimplerÂ treesÂ toÂ avoidÂ overfitting
Â Â Â Â use_label_encoder=False,
Â Â Â Â eval_metric='auc',
Â Â Â Â verbosity=0,
Â Â Â Â learning_rate=0.05,Â #Â HowÂ quicklyÂ theÂ modelÂ adaptsÂ toÂ theÂ trainingÂ dataÂ andÂ generalization.
Â Â Â Â min_child_weight=2,Â #Â TheÂ minimumÂ weightÂ (orÂ numberÂ ofÂ samplesÂ ifÂ allÂ samplesÂ haveÂ aÂ weightÂ ofÂ 1)Â requiredÂ inÂ orderÂ toÂ createÂ aÂ newÂ nodeÂ inÂ theÂ tree.
Â Â Â Â gamma=0,Â #Â RegularizationÂ techniqueÂ whichÂ ontrolsÂ treeÂ splitting.Â ItÂ isÂ theÂ minimumÂ lossÂ reductionÂ requiredÂ toÂ makeÂ aÂ furtherÂ splitÂ onÂ aÂ leafÂ node.Â HigherÂ gammaÂ =Â moreÂ conservativeÂ modelÂ (fewerÂ splits,Â lessÂ complexity,Â lessÂ overfitting).
Â Â Â Â random_state=42,
)

#Â TrainÂ theÂ modelÂ onÂ X_train,Â y_train
xgb_model.fit(train_X_resampled,Â train_y_resampled,Â verbose=False)

#Â MakeÂ predictions
y_predÂ =Â xgb_model.predict(valid_X)

#Â EvaluateÂ performance
print(classification_report(valid_y,Â y_pred))

joblib.dump(xgb_model,Â 'test.pkl')

!pipÂ installÂ streamlit

importÂ streamlitÂ asÂ st

%%writefileÂ app.py

st.title("ğŸ“ŠÂ XGBoostÂ ModelÂ Runner")

uploaded_fileÂ =Â st.file_uploader("UploadÂ yourÂ CSVÂ file",Â type=["csv"])

ifÂ uploaded_file:
Â Â Â Â dfÂ =Â pd.read_csv(uploaded_file)
Â Â Â Â st.subheader("PreviewÂ ofÂ UploadedÂ Data")
Â Â Â Â st.dataframe(df.head())

Â Â Â Â ifÂ st.button("RunÂ XGBoostÂ Model"):
Â Â Â Â Â Â Â Â try:
Â Â Â Â Â Â Â Â Â Â Â Â report_df,Â results_dfÂ =Â run_models(df)

Â Â Â Â Â Â Â Â Â Â Â Â st.subheader("ClassificationÂ Report")
Â Â Â Â Â Â Â Â Â Â Â Â st.dataframe(report_df)

Â Â Â Â Â Â Â Â Â Â Â Â #Â ExportÂ predictions
Â Â Â Â Â Â Â Â Â Â Â Â csvÂ =Â results_df.to_csv(index=False)
Â Â Â Â Â Â Â Â Â Â Â Â st.download_button(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â label="ğŸ“¥Â DownloadÂ ResultsÂ asÂ CSV",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â data=csv,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â file_name="model_results.csv",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â mime="text/csv"
Â Â Â Â Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â exceptÂ ExceptionÂ asÂ e:
Â Â Â Â Â Â Â Â Â Â Â Â st.error(f"ErrorÂ duringÂ modelÂ execution:Â {e}")

Writing app.py

!streamlitÂ runÂ app.py

!streamlitÂ runÂ /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py









